import streamlit as st
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Configuraci√≥n de la P√°gina ---
st.set_page_config(
    page_title="AI Fairness Playbooks",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

#======================================================================
# --- FUNCIONES DE SIMULACI√ìN ---
#======================================================================

def plot_simulation(df, title, x_label="Puntuaci√≥n del Modelo", y_label="Densidad"):
    """Funci√≥n auxiliar para graficar distribuciones."""
    fig, ax = plt.subplots()
    for group in df['Grupo'].unique():
        df[df['Grupo'] == group]['Puntuaci√≥n'].plot(kind='density', ax=ax, label=group)
    ax.set_title(title)
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)
    ax.legend()
    ax.grid(True, linestyle='--', alpha=0.6)
    st.pyplot(fig)

def run_threshold_simulation():
    """Simulaci√≥n para optimizaci√≥n de umbrales en post-procesamiento."""
    st.markdown("#### Simulaci√≥n de Optimizaci√≥n de Umbrales")
    st.write("Ajusta los umbrales de decisi√≥n para dos grupos y observa c√≥mo cambian las tasas de error para lograr la **Igualdad de Oportunidades** (tasas de verdaderos positivos iguales).")

    # Generar datos simulados
    np.random.seed(42)
    scores_a_pos = np.random.normal(0.7, 0.15, 80)
    scores_a_neg = np.random.normal(0.4, 0.15, 120)
    scores_b_pos = np.random.normal(0.6, 0.15, 50)
    scores_b_neg = np.random.normal(0.3, 0.15, 150)

    df_a = pd.DataFrame({'Puntuaci√≥n': np.concatenate([scores_a_pos, scores_a_neg]), 'Real': [1]*80 + [0]*120, 'Grupo': 'Grupo A'})
    df_b = pd.DataFrame({'Puntuaci√≥n': np.concatenate([scores_b_pos, scores_b_neg]), 'Real': [1]*50 + [0]*150, 'Grupo': 'Grupo B'})
    
    col1, col2 = st.columns(2)
    with col1:
        threshold_a = st.slider("Umbral para Grupo A", 0.0, 1.0, 0.5, key="sim_thresh_a")
    with col2:
        threshold_b = st.slider("Umbral para Grupo B", 0.0, 1.0, 0.5, key="sim_thresh_b")

    # Calcular m√©tricas
    tpr_a = np.mean(df_a[df_a['Real'] == 1]['Puntuaci√≥n'] >= threshold_a)
    fpr_a = np.mean(df_a[df_a['Real'] == 0]['Puntuaci√≥n'] >= threshold_a)
    
    tpr_b = np.mean(df_b[df_b['Real'] == 1]['Puntuaci√≥n'] >= threshold_b)
    fpr_b = np.mean(df_b[df_b['Real'] == 0]['Puntuaci√≥n'] >= threshold_b)

    st.markdown("##### Resultados")
    res_col1, res_col2 = st.columns(2)
    with res_col1:
        st.metric(label="Tasa de Verdaderos Positivos (Grupo A)", value=f"{tpr_a:.2%}")
        st.metric(label="Tasa de Falsos Positivos (Grupo A)", value=f"{fpr_a:.2%}")
    with res_col2:
        st.metric(label="Tasa de Verdaderos Positivos (Grupo B)", value=f"{tpr_b:.2%}")
        st.metric(label="Tasa de Falsos Positivos (Grupo B)", value=f"{fpr_b:.2%}")

    if abs(tpr_a - tpr_b) < 0.02:
        st.success(f"¬°Casi has logrado la Igualdad de Oportunidades! La diferencia en TPR es de solo {abs(tpr_a - tpr_b):.2%}.")
    else:
        st.warning(f"Ajusta los umbrales para igualar las Tasas de Verdaderos Positivos. Diferencia actual: {abs(tpr_a - tpr_b):.2%}")

#======================================================================
# --- FAIRNESS INTERVENTION PLAYBOOK ---
#======================================================================

def causal_fairness_toolkit():
    st.header("üõ°Ô∏è Toolkit de Equidad Causal")
    
    with st.expander("üîç Definici√≥n Amigable"):
        st.write("""
        El **An√°lisis Causal** va m√°s all√° de las correlaciones para entender el *porqu√©* de las disparidades. Es como ser un detective que no solo ve que dos eventos ocurren juntos, sino que reconstruye la cadena de causa y efecto que los conecta. Esto nos ayuda a aplicar soluciones que atacan la ra√≠z del problema, en lugar de solo maquillar los s√≠ntomas.
        """)
    
    tab1, tab2, tab3, tab4 = st.tabs(["Identificaci√≥n", "An√°lisis Contrafactual", "Diagrama Causal", "Inferencia Causal"])

    with tab1:
        st.subheader("Marco de Identificaci√≥n de Mecanismos de Discriminaci√≥n")
        st.info("Identifica las posibles causas ra√≠z del sesgo en tu aplicaci√≥n.")
        st.text_area("1. Discriminaci√≥n Directa: ¬øEl atributo protegido influye directamente en la decisi√≥n?", placeholder="Ej: ¬øSe utiliza expl√≠citamente el 'g√©nero' como una caracter√≠stica en el modelo?", key="c1")
        st.text_area("2. Discriminaci√≥n Indirecta: ¬øEl atributo protegido afecta a factores intermedios leg√≠timos?", placeholder="Ej: ¬øEl 'g√©nero' afecta a los 'a√±os de experiencia' debido a pausas en la carrera?", key="c2")
        st.text_area("3. Discriminaci√≥n por Proxy: ¬øLas decisiones dependen de variables correlacionadas con atributos protegidos?", placeholder="Ej: ¬øEl 'c√≥digo postal' se correlaciona con la 'raza' y se utiliza para predecir el riesgo?", key="c3")

    with tab2:
        st.subheader("Metodolog√≠a Pr√°ctica de Equidad Contrafactual")
        st.info("Analiza, cuantifica y mitiga el sesgo contrafactual en tu modelo.")
        with st.expander("üí° Ejemplo Interactivo: Simulaci√≥n Contrafactual"):
            st.write("Observa c√≥mo un cambio en un atributo protegido puede alterar la decisi√≥n de un modelo, revelando un sesgo causal.")
            
            # Simulaci√≥n
            puntaje_base = 650
            decision_base = "Rechazado"
            st.write(f"**Caso Base:** Solicitante del **Grupo B** con un puntaje de **{puntaje_base}**. Decisi√≥n del modelo: **{decision_base}**.")

            if st.button("Ver Contrafactual (Cambiar a Grupo A)", key="cf_button"):
                puntaje_cf = 710
                decision_cf = "Aprobado"
                st.info(f"**Escenario Contrafactual:** Mismo solicitante, pero del **Grupo A**. El modelo ahora predice un puntaje de **{puntaje_cf}** y la decisi√≥n es: **{decision_cf}**.")
                st.warning("**An√°lisis:** El cambio en el atributo protegido alter√≥ la decisi√≥n, lo que sugiere que el modelo ha aprendido una dependencia causal problem√°tica.")
        
        with st.container(border=True):
            st.markdown("##### Paso 1: An√°lisis de Equidad Contrafactual")
            st.text_area("1.1 Formular Consultas Contrafactuales", placeholder="Ej: Para un solicitante rechazado, ¬øcu√°l habr√≠a sido el resultado si su raza fuera diferente, manteniendo constantes los ingresos?", key="c4")
            st.text_area("1.2 Identificar Rutas Causales (Justas vs. Injustas)", placeholder="Ej: La ruta Raza -> C√≥digo Postal -> Decisi√≥n es injusta.", key="c5")
            st.text_area("1.3 Medir Disparidades y Documentar", placeholder="Ej: El 15% de los solicitantes del grupo desfavorecido habr√≠an sido aprobados en el escenario contrafactual.", key="c6")
        with st.container(border=True):
            st.markdown("##### Paso 2: An√°lisis Espec√≠fico de Rutas")
            st.text_area("2.1 Descomponer y Clasificar Rutas", placeholder="Ej: Ruta 1 (proxy de c√≥digo postal) clasificada como INJUSTA.", key="c7")
            st.text_area("2.2 Cuantificar Contribuci√≥n y Documentar", placeholder="Ej: La ruta del c√≥digo postal representa el 60% de la disparidad observada.", key="c8")
        with st.container(border=True):
            st.markdown("##### Paso 3: Dise√±o de Intervenci√≥n")
            st.selectbox("3.1 Seleccionar Enfoque de Intervenci√≥n", ["Nivel de Datos", "Nivel de Modelo", "Post-procesamiento"], key="c9")
            st.text_area("3.2 Implementar y Monitorear", placeholder="Ej: Se aplic√≥ una transformaci√≥n a la caracter√≠stica de c√≥digo postal. La disparidad se redujo en un 50%.", key="c10")

    with tab3:
        st.subheader("Enfoque de Diagrama Causal Inicial")
        st.info("Esboza diagramas para visualizar las relaciones causales y documentar tus supuestos.")
        st.markdown("""
        **Convenciones de Anotaci√≥n:**
        - **Nodos (variables):** Atributos Protegidos, Caracter√≠sticas, Resultados.
        - **Flechas Causales (‚Üí):** Relaci√≥n causal asumida.
        - **Flechas de Correlaci√≥n (<-->):** Correlaci√≥n sin causalidad directa conocida.
        - **Incertidumbre (?):** Relaci√≥n causal hipot√©tica o d√©bil.
        - **Ruta Problem√°tica (!):** Ruta que consideras una fuente de inequidad.
        """)
        st.text_area("Documentaci√≥n de Supuestos y Rutas", placeholder="Ruta (!): Raza -> Nivel de Ingresos -> Decisi√≥n.\nSupuesto: Las disparidades hist√≥ricas de ingresos vinculadas a la raza afectan la capacidad de pr√©stamo.", height=200, key="c11")

    with tab4:
        st.subheader("Inferencia Causal con Datos Limitados")
        st.info("M√©todos pr√°cticos para estimar efectos causales cuando los datos son imperfectos.")
        st.markdown("""
        **M√©todos de Inferencia Observacional:**
        - **Matching (Emparejamiento):** Compara individuos similares de diferentes grupos.
        - **Variables Instrumentales (IV):** Usa una variable externa para aislar el efecto causal.
        - **Regresi√≥n por Discontinuidad:** Aprovecha umbrales naturales en los datos.
        - **Diferencia en Diferencias:** Compara cambios en el tiempo entre grupos.
        """)
        st.text_area("An√°lisis de Sensibilidad", placeholder="¬øQu√© tan fuerte tendr√≠a que ser una variable de confusi√≥n no medida para anular el efecto de sesgo que has encontrado?", key="c12")

def preprocessing_fairness_toolkit():
    st.header("üß™ Toolkit de Equidad en Pre-procesamiento")
    with st.expander("üîç Definici√≥n Amigable"):
        st.write("""
        El **Pre-procesamiento** consiste en "limpiar" los datos *antes* de que el modelo aprenda de ellos. Es como preparar los ingredientes para una receta: si sabes que algunos ingredientes est√°n sesgados (por ejemplo, demasiado salados), los ajustas antes de cocinar para asegurar que el plato final sea equilibrado.
        """)
    
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["An√°lisis de Representaci√≥n", "Detecci√≥n de Correlaci√≥n", "Calidad de Etiquetas", "Re-ponderaci√≥n y Re-muestreo", "Transformaci√≥n", "Generaci√≥n de Datos"])

    with tab1:
        st.subheader("An√°lisis de Representaci√≥n Multidimensional")
        st.info("Examina las distribuciones demogr√°ficas para identificar brechas de representaci√≥n.")
        st.text_area("1. Comparaci√≥n con Poblaci√≥n de Referencia", placeholder="Ej: Nuestro conjunto de datos tiene un 20% de mujeres en roles t√©cnicos, mientras que el mercado laboral es del 35%.", key="p1")
        st.text_area("2. An√°lisis de Representaci√≥n Interseccional", placeholder="Ej: Las mujeres de minor√≠as raciales constituyen solo el 3% de los datos.", key="p2")
        st.text_area("3. Representaci√≥n a trav√©s de Categor√≠as de Resultados", placeholder="Ej: El grupo A constituye el 30% de las solicitudes pero solo el 10% de las aprobadas.", key="p3")

    with tab2:
        st.subheader("Detecci√≥n de Patrones de Correlaci√≥n")
        st.info("Identifica asociaciones problem√°ticas que podr√≠an permitir la discriminaci√≥n por proxy.")
        st.text_area("1. Correlaciones Directas (Atributo Protegido ‚Üî Resultado)", placeholder="Ej: El g√©nero tiene una correlaci√≥n de 0.3 con la decisi√≥n de contrataci√≥n.", key="p4")
        st.text_area("2. Identificaci√≥n de Variables Proxy (Atributo Protegido ‚Üî Caracter√≠stica)", placeholder="Ej: La caracter√≠stica 'asistencia a un club de ajedrez' est√° altamente correlacionada con el g√©nero masculino.", key="p5")

    with tab3:
        st.subheader("Evaluaci√≥n de la Calidad de las Etiquetas")
        st.info("Eval√∫a los sesgos potenciales en las etiquetas de entrenamiento.")
        st.text_area("1. Sesgo Hist√≥rico en las Decisiones", placeholder="Ej: Las etiquetas de 'promocionado' provienen de un per√≠odo con pol√≠ticas de promoci√≥n sesgadas.", key="p6")
        st.text_area("2. Sesgo del Anotador", placeholder="Ej: Los anotadores masculinos calificaron los mismos comentarios como 't√≥xicos' con menos frecuencia que las anotadoras femeninas.", key="p7")
    
    with tab4:
        st.subheader("T√©cnicas de Re-ponderaci√≥n y Re-muestreo")
        with st.expander("üí° Ejemplo Interactivo: Simulaci√≥n de Sobremuestreo"):
            st.write("Observa c√≥mo el sobremuestreo (resampling) puede equilibrar un conjunto de datos con representaci√≥n desigual.")
            np.random.seed(0)
            data_a = np.random.multivariate_normal([2, 2], [[1, .5], [.5, 1]], 100)
            data_b = np.random.multivariate_normal([4, 4], [[1, .5], [.5, 1]], 20)
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # Gr√°fico Original
            ax1.scatter(data_a[:, 0], data_a[:, 1], c='blue', label='Grupo A (n=100)', alpha=0.6)
            ax1.scatter(data_b[:, 0], data_b[:, 1], c='red', label='Grupo B (n=20)', alpha=0.6)
            ax1.set_title("Datos Originales (Desequilibrados)")
            ax1.legend()
            ax1.grid(True, linestyle='--', alpha=0.5)

            # Gr√°fico con Sobremuestreo
            oversample_indices = np.random.choice(range(20), 80, replace=True)
            data_b_oversampled = np.vstack([data_b, data_b[oversample_indices]])
            ax2.scatter(data_a[:, 0], data_a[:, 1], c='blue', label='Grupo A (n=100)', alpha=0.6)
            ax2.scatter(data_b_oversampled[:, 0], data_b_oversampled[:, 1], c='red', label='Grupo B (n=100)', alpha=0.6, marker='x')
            ax2.set_title("Datos con Sobremuestreo del Grupo B")
            ax2.legend()
            ax2.grid(True, linestyle='--', alpha=0.5)
            
            st.pyplot(fig)
            st.info("El gr√°fico de la derecha muestra c√≥mo se han a√±adido nuevas muestras (marcadas con 'x') del Grupo B para igualar en n√∫mero al Grupo A, lo que ayuda al modelo a aprender mejor sus patrones.")
        st.info("Aborda las disparidades de representaci√≥n ajustando la influencia de las instancias de entrenamiento.")
        st.markdown("**Re-ponderaci√≥n:** Asigna pesos a las muestras para dar m√°s importancia a los grupos subrepresentados.")
        st.markdown("**Re-muestreo:** Modifica f√≠sicamente el conjunto de datos (sobre-muestreo o sub-muestreo).")
        st.text_area("Criterios de Decisi√≥n: ¬øRe-ponderar o Re-muestrear?", placeholder="Basado en mi auditor√≠a y mi modelo, la mejor estrategia es...", key="p8")
        st.text_area("Consideraci√≥n de Interseccionalidad", placeholder="Mi plan para la interseccionalidad es...", key="p9")

    with tab5:
        st.subheader("Enfoques de Transformaci√≥n de Distribuci√≥n")
        st.info("Modifica el espacio de caracter√≠sticas para mitigar el sesgo.")
        st.text_area("1. Eliminaci√≥n de Impacto Dispar", placeholder="Ej: 'Reparar' la caracter√≠stica 'c√≥digo postal' para que su distribuci√≥n sea la misma en todos los grupos raciales.", key="p10")
        st.text_area("2. Representaciones Justas (LFR, LAFTR)", placeholder="Ej: Usar un autoencoder adversario para aprender una representaci√≥n que no contenga informaci√≥n de g√©nero.", key="p11")
        st.text_area("3. Consideraciones de Interseccionalidad", placeholder="Mi estrategia de transformaci√≥n se centrar√° en las intersecciones de g√©nero y etnia...", key="p12")

    with tab6:
        st.subheader("Generaci√≥n de Datos con Conciencia de Equidad")
        st.info("Crea datos sint√©ticos para mitigar patrones de sesgo.")
        st.markdown("**¬øCu√°ndo Generar Datos?:** Cuando hay subrepresentaci√≥n severa o se necesitan ejemplos contrafactuales.")
        st.markdown("**Estrategias:** Generaci√≥n Condicional, Aumentaci√≥n Contrafactual.")
        st.text_area("Consideraciones de Interseccionalidad", placeholder="Mi modelo generativo ser√° condicionado en la intersecci√≥n de edad y g√©nero para...", key="p13")

def inprocessing_fairness_toolkit():
    st.header("‚öôÔ∏è Toolkit de Equidad en In-procesamiento")
    with st.expander("üîç Definici√≥n Amigable"):
        st.write("""
        El **In-procesamiento** implica modificar el algoritmo de aprendizaje del modelo para que la equidad sea uno de sus objetivos, junto con la precisi√≥n. Es como ense√±arle a un chef a cocinar no solo para que la comida sea deliciosa, sino tambi√©n para que sea nutricionalmente equilibrada, haciendo de la nutrici√≥n una parte central de la receta.
        """)
    
    tab1, tab2, tab3, tab4 = st.tabs(["Objetivos y Restricciones", "Debiasing Adversario", "Optimizaci√≥n Multiobjetivo", "Patrones de C√≥digo"])

    with tab1:
        st.subheader("Objetivos y Restricciones de Equidad")
        st.info("Incorpora la equidad directamente en la optimizaci√≥n del modelo.")
        st.markdown("**M√©todos Lagrangianos:** Transforma restricciones duras en penalizaciones suaves en la funci√≥n de p√©rdida.")
        st.latex(r''' \mathcal{L}(\theta, \lambda) = L(\theta) + \sum_{i=1}^{k} \lambda_i C_i(\theta) ''')
        st.markdown("**Viabilidad y Compensaciones:** Entiende la tensi√≥n entre equidad y rendimiento.")
        st.markdown("**Interseccionalidad:** Las restricciones deben considerar combinaciones de atributos.")

    with tab2:
        st.subheader("Enfoques de Debiasing Adversario")
        st.info("Usa aprendizaje adversario para que los modelos 'desaprendan' patrones discriminatorios.")
        st.markdown("**Arquitectura:** Un **Predictor** compite contra un **Adversario**. El predictor aprende a enga√±ar al adversario, creando representaciones sin informaci√≥n del atributo protegido.")
        st.markdown("**Optimizaci√≥n:** El entrenamiento puede ser inestable. Requiere equilibrio de componentes, inversi√≥n de gradiente y entrenamiento progresivo.")
    
    with tab3:
        st.subheader("Optimizaci√≥n Multiobjetivo para la Equidad")
        with st.expander("üí° Ejemplo Interactivo: Frontera de Pareto"):
            st.write("Explora la **frontera de Pareto**, que visualiza la compensaci√≥n (trade-off) entre la precisi√≥n de un modelo y su equidad. No se puede mejorar uno sin empeorar el otro.")
            
            # Datos simulados para la frontera
            np.random.seed(10)
            accuracy = np.linspace(0.80, 0.95, 20)
            fairness_score = 1 - np.sqrt(accuracy - 0.79) + np.random.normal(0, 0.02, 20)
            fairness_score = np.clip(fairness_score, 0.5, 1.0)
            
            fig, ax = plt.subplots()
            ax.scatter(accuracy, fairness_score, c=accuracy, cmap='viridis', label='Modelos Posibles')
            ax.set_title("Frontera de Pareto: Equidad vs. Precisi√≥n")
            ax.set_xlabel("Precisi√≥n del Modelo")
            ax.set_ylabel("Puntuaci√≥n de Equidad")
            ax.grid(True, linestyle='--', alpha=0.6)
            st.pyplot(fig)
            st.info("Cada punto representa un modelo diferente. Los modelos en el borde superior derecho son '√≥ptimos': no puedes encontrar un modelo m√°s justo sin sacrificar precisi√≥n, y viceversa. La elecci√≥n de qu√© punto usar depende de las prioridades de tu proyecto.")
        st.info("Navega sistem√°ticamente las tensiones entre equidad y rendimiento.")
        st.text_area("An√°lisis y Definici√≥n de Objetivos", placeholder="Define tus m√©tricas de rendimiento y criterios de equidad aqu√≠.", key="i1")

    with tab4:
        st.subheader("Cat√°logo de Patrones de Implementaci√≥n")
        st.code("""
def fairness_regularized_loss(original_loss, predictions, protected_attribute):
  fairness_penalty = calculate_disparity(predictions, protected_attribute)
  return original_loss + lambda * fairness_penalty
        """, language="python")

def postprocessing_fairness_toolkit():
    st.header("üìä Toolkit de Equidad en Post-procesamiento")
    with st.expander("üîç Definici√≥n Amigable"):
        st.write("""
        El **Post-procesamiento** consiste en ajustar las predicciones de un modelo *despu√©s* de que ya ha sido entrenado. Es como un editor que revisa un texto ya escrito para corregir sesgos o errores. El modelo original no cambia, solo se ajusta su resultado final para que sea m√°s justo.
        """)
    
    tab1, tab2, tab3, tab4 = st.tabs(["Optimizaci√≥n de Umbrales", "Calibraci√≥n", "Transformaci√≥n de Predicci√≥n", "Clasificaci√≥n con Rechazo"])

    with tab1:
        st.subheader("T√©cnicas de Optimizaci√≥n de Umbrales")
        with st.expander("üí° Ejemplo Interactivo"):
             run_threshold_simulation()
        st.info("Ajusta los umbrales de clasificaci√≥n despu√©s del entrenamiento para satisfacer definiciones de equidad espec√≠ficas.")
        st.text_area("Metodolog√≠a de Implementaci√≥n", placeholder="1. Seleccionar Criterio de Equidad (ej. igualdad de oportunidades)\n2. Calcular Umbrales en datos de validaci√≥n\n3. Analizar Compensaciones y Desplegar", key="po1")

    with tab2:
        st.subheader("Gu√≠a Pr√°ctica de Calibraci√≥n para la Equidad")
        st.info("Garantiza que las probabilidades predichas tengan un significado consistente en todos los grupos.")
        st.markdown("**Fundamentos:** Una mala calibraci√≥n significa que la misma puntuaci√≥n de riesgo representa diferentes niveles de riesgo real para diferentes grupos.")
        st.markdown("**T√©cnicas:** Platt Scaling, Regresi√≥n Isot√≥nica.")
        st.text_area("Metodolog√≠a de Implementaci√≥n", placeholder="1. Evaluar Calibraci√≥n (con ECE, MCE)\n2. Seleccionar M√©todo\n3. Implementar y Validar", key="po2")

    with tab3:
        st.subheader("M√©todos de Transformaci√≥n de Predicci√≥n")
        st.info("Modifica las salidas del modelo para satisfacer restricciones de equidad complejas.")
        st.markdown("**Conceptos Clave:** Funciones de Transformaci√≥n Aprendidas, Alineaci√≥n de Distribuci√≥n, Transformaciones de Puntuaci√≥n Justas.")
        st.text_area("Metodolog√≠a de Implementaci√≥n", placeholder="1. Dise√±ar Transformaci√≥n\n2. Aprender y Evaluar\n3. Considerar Interseccionalidad", key="po3")

    with tab4:
        st.subheader("Clasificaci√≥n con Opci√≥n de Rechazo")
        st.info("Identifica predicciones inciertas y las difiere a juicio humano.")
        st.markdown("**Fundamentos:** Umbrales de rechazo basados en confianza, clasificaci√≥n selectiva, modelos de colaboraci√≥n Humano-IA.")
        st.text_area("Metodolog√≠a de Implementaci√≥n", placeholder="1. Estimar Confianza\n2. Optimizar Umbral de Rechazo\n3. Dise√±ar Flujo de Trabajo Humano-IA", key="po4")

def intervention_playbook():
    st.sidebar.title("Navegaci√≥n del Playbook de Intervenci√≥n")
    selection = st.sidebar.radio(
        "Ir a:",
        ["Playbook Principal", "Toolkit Causal", "Toolkit de Pre-procesamiento", "Toolkit de In-procesamiento", "Toolkit de Post-procesamiento"],
        key="intervention_nav"
    )
    
    if selection == "Playbook Principal":
        st.header("üìñ Playbook de Intervenci√≥n de Equidad")
        st.info("Este playbook integra los cuatro toolkits en un flujo de trabajo cohesivo, guiando a los desarrolladores desde la identificaci√≥n del sesgo hasta la implementaci√≥n de soluciones efectivas.")
        with st.expander("Gu√≠a de Implementaci√≥n"):
            st.write("Explica c√≥mo usar el playbook, con comentarios sobre puntos de decisi√≥n clave, evidencia de apoyo y riesgos identificados.")
        with st.expander("Estudio de Caso"):
            st.write("Demuestra la aplicaci√≥n del playbook a un problema de equidad t√≠pico, mostrando c√≥mo los resultados de cada componente informan al siguiente.")
        with st.expander("Marco de Validaci√≥n"):
            st.write("Proporciona orientaci√≥n sobre c√≥mo los equipos de implementaci√≥n pueden verificar la efectividad de su proceso de auditor√≠a.")
        with st.expander("Equidad Interseccional"):
            st.write("Consideraci√≥n expl√≠cita de la equidad interseccional en cada componente del playbook.")
    elif selection == "Toolkit Causal":
        causal_fairness_toolkit()
    elif selection == "Toolkit de Pre-procesamiento":
        preprocessing_fairness_toolkit()
    elif selection == "Toolkit de In-procesamiento":
        inprocessing_fairness_toolkit()
    elif selection == "Toolkit de Post-procesamiento":
        postprocessing_fairness_toolkit()

#======================================================================
# --- FAIRNESS AUDIT PLAYBOOK ---
#======================================================================

def audit_playbook():
    st.sidebar.title("Navegaci√≥n del Playbook de Auditor√≠a")
    page = st.sidebar.radio("Ir a", [
        "C√≥mo Navegar este Playbook",
        "Evaluaci√≥n del Contexto Hist√≥rico",
        "Selecci√≥n de Definici√≥n de Equidad",
        "Identificaci√≥n de Fuentes de Sesgo",
        "M√©tricas Comprensivas de Equidad"
    ], key="audit_nav")

    if page == "C√≥mo Navegar este Playbook":
        st.header("C√≥mo Navegar Este Playbook")
        st.markdown("""
        **El Marco de Cuatro Componentes** ‚Äì Sigue secuencialmente a trav√©s de:
        
        1. **Evaluaci√≥n del Contexto Hist√≥rico (HCA)** ‚Äì Descubre sesgos sist√©micos y desequilibrios de poder en tu dominio.
        
        2. **Selecci√≥n de Definici√≥n de Equidad (FDS)**
         ‚Äì Elige las definiciones de equidad apropiadas basadas en tu contexto y objetivos.
        
        3. **Identificaci√≥n de Fuentes de Sesgo (BSI)** ‚Äì Identifica y prioriza las formas en que el sesgo puede entrar en tu sistema.
        
        4. **M√©tricas Comprensivas de Equidad (CFM)**
         ‚Äì Implementa m√©tricas cuantitativas para el monitoreo y la presentaci√≥n de informes.

        **Consejos:**
        - Avanza por las secciones en orden, pero si√©ntete libre de retroceder si surgen nuevas ideas.
        - Usa los botones de **Guardar Resumen** en cada herramienta para registrar tus hallazgos.
        - Consulta los ejemplos incrustados en cada secci√≥n para ver c√≥mo otros han aplicado estas herramientas.
        """)
    elif page == "Evaluaci√≥n del Contexto Hist√≥rico":
        st.header("Herramienta de Evaluaci√≥n del Contexto Hist√≥rico")
        with st.expander("üîç Definici√≥n Amigable"):
            st.write("""
            El **Contexto Hist√≥rico** es el trasfondo social y cultural en el que se utilizar√° tu IA. Es importante porque los sesgos no nacen en los algoritmos, sino en la sociedad. Entender la historia de la discriminaci√≥n en √°reas como la banca o la contrataci√≥n nos ayuda a anticipar d√≥nde nuestra IA podr√≠a fallar y perpetuar injusticias pasadas.
            """)
        st.subheader("1. Cuestionario Estructurado")
        st.markdown("Esta secci√≥n te ayuda a descubrir patrones relevantes de discriminaci√≥n hist√≥rica.")
        
        q1 = st.text_area("¬øEn qu√© dominio espec√≠fico operar√° este sistema (ej. pr√©stamos, contrataci√≥n, salud)?")
        q2 = st.text_area("¬øCu√°l es la funci√≥n espec√≠fica del sistema o caso de uso dentro de ese dominio?")
        q3 = st.text_area("¬øCu√°les son los patrones de discriminaci√≥n hist√≥rica documentados en este dominio?")
        q4 = st.text_area("¬øQu√© fuentes de datos hist√≥ricos se utilizan o se referencian en este sistema?")
        q5 = st.text_area("¬øC√≥mo se definieron hist√≥ricamente las categor√≠as clave (ej. g√©nero, riesgo crediticio) y han evolucionado?")
        q6 = st.text_area("¬øC√≥mo se midieron hist√≥ricamente las variables (ej. ingresos, educaci√≥n)? ¬øPodr√≠an codificar sesgos?")
        q7 = st.text_area("¬øHan servido otras tecnolog√≠as para roles similares en este dominio? ¬øDesafiaron o reforzaron las desigualdades?")
        q8 = st.text_area("¬øC√≥mo podr√≠a la automatizaci√≥n amplificar los sesgos pasados o introducir nuevos riesgos en este dominio?")

        st.subheader("2. Matriz de Clasificaci√≥n de Riesgos")
        st.markdown("""
        Para cada patr√≥n hist√≥rico identificado, estima:
        - **Severidad**: Alto = impacta derechos/resultados de vida, Medio = afecta oportunidades/acceso a recursos, Bajo = impacto material limitado.
        - **Probabilidad**: Alta = probable que aparezca en sistemas similares, Media = posible, Baja = raro.
        - **Relevancia**: Alta = directamente relacionado con tu sistema, Media = afecta partes, Baja = perif√©rico.
        """)
        matrix = st.text_area("Matriz de Clasificaci√≥n de Riesgos (tabla Markdown)", height=200, placeholder="| Patr√≥n | Severidad | Probabilidad | Relevancia | Puntuaci√≥n (S√óP√óR) | Prioridad |\n|---|---|---|---|---|---|")

        if st.button("Guardar Resumen HCA"):
            summary = {
                "Cuestionario Estructurado": {
                    "Dominio": q1, "Funci√≥n": q2, "Patrones Hist√≥ricos": q3, "Fuentes de Datos": q4,
                    "Definiciones de Categor√≠a": q5, "Riesgos de Medici√≥n": q6, "Sistemas Anteriores": q7, "Riesgos de Automatizaci√≥n": q8
                },
                "Matriz de Riesgos": matrix
            }
            summary_md = "# Resumen de Evaluaci√≥n del Contexto Hist√≥rico\n"
            for section, answers in summary.items():
                summary_md += f"## {section}\n"
                if isinstance(answers, dict):
                    for k, v in answers.items():
                        summary_md += f"**{k}:** {v}\n\n"
                else:
                    summary_md += f"{answers}\n"
            
            st.subheader("Vista Previa del Resumen HCA")
            st.markdown(summary_md)
            st.download_button("Descargar Resumen HCA", summary_md, "HCA_summary.md", "text/markdown")
            st.success("Resumen de Evaluaci√≥n del Contexto Hist√≥rico guardado.")

    elif page == "Selecci√≥n de Definici√≥n de Equidad":
        st.header("Herramienta de Selecci√≥n de Definici√≥n de Equidad")
        with st.expander("üîç Definici√≥n Amigable"):
            st.write("""
            No existe una √∫nica "receta" para la equidad. Diferentes situaciones requieren diferentes tipos de justicia. Esta secci√≥n te ayuda a elegir la **definici√≥n de equidad** m√°s adecuada para tu proyecto, como un m√©dico que elige el tratamiento correcto para una enfermedad espec√≠fica. Algunas definiciones buscan igualdad de resultados, otras igualdad de oportunidades, y la elecci√≥n correcta depende de tu objetivo y del da√±o que intentas evitar.
            """)
        st.subheader("1. Cat√°logo de Definiciones de Equidad")
        st.markdown("""
        | Definici√≥n | F√≥rmula | Cu√°ndo Usar | Ejemplo |
        |---|---|---|---|
        | Paridad Demogr√°fica | P(≈∂=1|A=a) = P(≈∂=1|A=b) | Asegurar tasas de positivos iguales entre grupos. | Anuncios de universidad mostrados por igual a todos los g√©neros. |
        | Igualdad de Oportunidades | P(≈∂=1|Y=1,A=a) = P(≈∂=1|Y=1,A=b) | Minimizar falsos negativos entre individuos calificados. | Sensibilidad de prueba m√©dica igual entre razas. |
        | Probabilidades Igualadas | P(≈∂=1|Y=y,A=a) = P(≈∂=1|Y=y,A=b) ‚àÄ y | Equilibrar falsos positivos y negativos entre grupos. | Predicciones de reincidencia con tasas de error iguales. |
        | Calibraci√≥n | P(Y=1|≈ù=s,A=a) = s | Cuando las puntuaciones predichas se exponen a los usuarios. | Puntuaciones de cr√©dito calibradas para diferentes demograf√≠as. |
        | Equidad Contrafactual | ≈∂(x) = ≈∂(x') si A cambia | Requerir eliminaci√≥n de sesgo causal relativo a rasgos sensibles. | Resultado sin cambios si solo cambia la raza en el perfil. |
        """)
        st.subheader("2. √Årbol de Decisi√≥n para Selecci√≥n")
        exclusion = st.radio("¬øEl HCA revel√≥ exclusi√≥n sist√©mica de grupos protegidos?", ("S√≠", "No"), key="fds1")
        error_harm = st.radio("¬øQu√© tipo de error es m√°s da√±ino en tu contexto?", ("Falsos Negativos", "Falsos Positivos", "Ambos por igual"), key="fds2")
        score_usage = st.checkbox("¬øSe usar√°n las salidas como puntuaciones (ej. riesgo, ranking)?", key="fds3")
        
        st.subheader("Definiciones Recomendadas")
        definitions = []
        if exclusion == "S√≠": definitions.append("Paridad Demogr√°fica")
        if error_harm == "Falsos Negativos": definitions.append("Igualdad de Oportunidades")
        elif error_harm == "Falsos Positivos": definitions.append("Igualdad Predictiva")
        elif error_harm == "Ambos por igual": definitions.append("Probabilidades Igualadas")
        if score_usage: definitions.append("Calibraci√≥n")
        
        for d in definitions: st.markdown(f"- **{d}**")

    # (El resto de las secciones del Audit Playbook se pueden a√±adir aqu√≠ de manera similar)
    elif page == "Identificaci√≥n de Fuentes de Sesgo":
        st.header("Herramienta de Identificaci√≥n de Fuentes de Sesgo")
        # (Contenido de esta secci√≥n)
    elif page == "M√©tricas Comprensivas de Equidad":
        st.header("M√©tricas Comprensivas de Equidad (CFM)")
        # (Contenido de esta secci√≥n)


# --- NAVEGACI√ìN PRINCIPAL ---
st.sidebar.title("Selecci√≥n de Playbook")
playbook_choice = st.sidebar.selectbox(
    "Elige el playbook que quieres usar:",
    ["Fairness Audit Playbook", "Fairness Intervention Playbook"]
)

st.title(playbook_choice)

if playbook_choice == "Fairness Audit Playbook":
    audit_playbook() 
else:
    intervention_playbook()
